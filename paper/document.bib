@misc{nielsenneural,
	title={Neural networks and deep learning},
	author={Nielsen, Michael A},
	year={2015},
	publisher={Determination Press},
	url={http://neuralnetworksanddeeplearning.com/},
}

@misc{sklearn-newsgroup,
	title = {5.6.2. The 20 newsgroups text dataset - scikit-learn 0.19.1 documentation},
	howpublished = {\url{http://scikit-learn.org/stable/datasets/twenty_newsgroups.html}},
	note = {Accessed: 2018-06-11},
	year = "2018"	
}

@misc{stemming,
	title = {Machine Learning, NLP: Text Classification using scikit-learn, python and NLTK.},
	howpublished = {\url{https://towardsdatascience.com/machine-learning-nlp-text-classification-using-scikit-learn-python-and-nltk-c52b92a7c73a}},
	note = {Accessed: 2018-06-12},
	year = "2017"
}

@article{scikit-learn,
 title={Scikit-learn: Machine Learning in {P}ython},
 author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
         and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
         and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
         Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
 journal={Journal of Machine Learning Research},
 volume={12},
 pages={2825--2830},
 year={2011}
}

@InProceedings{pmlr-v15-glorot11a,
	title = 	 {Deep Sparse Rectifier Neural Networks},
	author = 	 {Xavier Glorot and Antoine Bordes and Yoshua Bengio},
	booktitle = 	 {Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics},
	pages = 	 {315--323},
	year = 	 {2011},
	editor = 	 {Geoffrey Gordon and David Dunson and Miroslav Dud√≠k},
	volume = 	 {15},
	series = 	 {Proceedings of Machine Learning Research},
	address = 	 {Fort Lauderdale, FL, USA},
	month = 	 {11--13 Apr},
	publisher = 	 {PMLR},
	pdf = 	 {http://proceedings.mlr.press/v15/glorot11a/glorot11a.pdf},
	url = 	 {http://proceedings.mlr.press/v15/glorot11a.html},
	abstract = 	 {While logistic sigmoid neurons are more biologically plausible than hyperbolic tangent neurons, the latter work better for training multi-layer neural networks. This paper shows that rectifying neurons are an even better model of biological neurons and yield equal or better performance than hyperbolic tangent networks in spite of the hard non-linearity and non-differentiability at zero, creating sparse representations with true zeros which seem remarkably suitable for naturally sparse data. Even though they can take advantage of semi-supervised setups with extra-unlabeled data, deep rectifier networks can reach their best performance without requiring any unsupervised pre-training on purely supervised tasks with large labeled datasets. Hence, these results can be seen as a new milestone in the attempts at understanding the difficulty in training deep but purely supervised neural networks, and closing the performance gap between neural networks learnt with and without unsupervised pre-training. [pdf]}
}

@ARTICLE{adam,
	author = {{Kingma}, D.~P. and {Ba}, J.},
	title = "{Adam: A Method for Stochastic Optimization}",
	journal = {ArXiv e-prints},
	archivePrefix = "arXiv",
	eprint = {1412.6980},
	primaryClass = "cs.LG",
	keywords = {Computer Science - Learning},
	year = 2014,
	month = dec,
	adsurl = {http://adsabs.harvard.edu/abs/2014arXiv1412.6980K},
	adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@book{introduction_ml,
	title={Introduction to Machine Learning with Python: A Guide for Data Scientists},
	author={M{\"u}ller, A.C. and Guido, S.},
	isbn={9781449369897},
	url={https://books.google.de/books?id=vbQlDQAAQBAJ},
	year={2016},
	publisher={O'Reilly Media}
}

@inproceedings{rcnn,
	author = {Lai, Siwei and Xu, Liheng and Liu, Kang and Zhao, Jun},
	title = {Recurrent Convolutional Neural Networks for Text Classification},
	booktitle = {Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence},
	series = {AAAI'15},
	year = {2015},
	isbn = {0-262-51129-0},
	location = {Austin, Texas},
	pages = {2267--2273},
	numpages = {7},
	url = {http://dl.acm.org/citation.cfm?id=2886521.2886636},
	acmid = {2886636},
	publisher = {AAAI Press},
}
