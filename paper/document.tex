\documentclass[sigconf]{acmart}

\usepackage{booktabs} % For formal tables
\usepackage{hyperref}
\usepackage{listings}
\usepackage{graphicx}


% Copyright
%\setcopyright{none}
%\setcopyright{acmcopyright}
%\setcopyright{acmlicensed}
\setcopyright{rightsretained}
%\setcopyright{usgov}
%\setcopyright{usgovmixed}
%\setcopyright{cagov}
%\setcopyright{cagovmixed}


% DOI
% \acmDOI{10.475/123_4}

% ISBN
% \acmISBN{123-4567-24-567/08/06}

%Conference
\acmConference[UPDS-SoSe18]{Uni Passau - Data Science Seminar - SoSe18}{July 2018}{Passau, Germany}
\copyrightyear{2018}

\begin{document}
\title{Hyperparameter Study for Classical Feedforward Neural Networks on Text Data}

\author{Marius Kleidl}
\affiliation{%
  \institution{University of Passau}
  \city{Passau}
}
\email{kleidl01@gw.uni-passau.de}


% The default list of authors is too long for headers.
\renewcommand{\shortauthors}{B. Trovato et al.}


\begin{abstract}
	In this paper we train and evaluate the accuracy of text classifiers using simple feed-forward neural networks with different learning rates, number of hidden layers and numbers of perceptrons per layer. Our results show that different values for these hyperparameters influence the classifier's accuracy while their impact on the performance is limited . We conclude that better text preprocessing and feature selection may be necessary to further improve the classifier.
\end{abstract}

%
% The code below should be generated by the tool at
% http://dl.acm.org/ccs.cfm
% Please copy and paste the code instead of the example below.
%
\begin{CCSXML}
	<ccs2012>
	<concept>
	<concept_id>10010147.10010257.10010258.10010259.10010263</concept_id>
	<concept_desc>Computing methodologies~Supervised learning by classification</concept_desc>
	<concept_significance>500</concept_significance>
	</concept>
	<concept>
	<concept_id>10010147.10010257.10010293.10010294</concept_id>
	<concept_desc>Computing methodologies~Neural networks</concept_desc>
	<concept_significance>500</concept_significance>
	</concept>
	</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Computing methodologies~Supervised learning by classification}
\ccsdesc[500]{Computing methodologies~Neural networks}

% TODO
\keywords{ACM proceedings, \LaTeX, text tagging}

\maketitle

\section{Introduction}

In Natural Language Processing application the task of classifying text is an common one and is widely used in areas such as sentiment analysis, recommendation engines and content filtering. Neural networks have received increasing attention for solving these complex problems. 

\section{Contribution}

In our experiment we train and evaluate the performance of multiple deep neural networks with different numbers of layers, numbers of neurons per layer and different learning rates.
This section describes the setup of our experiment, in particular the data set, the chosen feature extraction steps, the hyperparameters for our neural network and our evaluation method. The experiment is conducted using the machine learning software scikit-learn\cite{scikit-learn} in its version 0.19.1.

\subsection{Text Data Set}

The experiments are performed using the well-known 20Newsgroups\footnote{\href{http://qwone.com/~jason/20Newsgroups/}{http://qwone.com/$\sim$jason/20Newsgroups/}} data set. It contains 18846 postings in the English language which are nearly evenly distributed across 20 different newsgroups, as can be seen in Table~\ref{tab:groups}. Furthermore, the set has been split into a training set of 11314 postings and a testing set of 7532. This division is based upon the time of the posting\cite{sklearn-newsgroup}.

An example for a post's format is given in Figure~\ref{fig:post}. As one can see, the post contains a header and signature sections consisting of additional meta data, such as the sender's e-mail address and organization. To prevent our neural network from association this meta data with specific classes, we remove the headers and signatures from each post.

\begin{figure*}
	\label{fig:post}
	\begin{center}
		\begin{tabular}{c}
			\begin{lstlisting}[basicstyle=\footnotesize]
From: JC924@uacsc2.albany.edu
Subject: Why are our desktop fonts changing?
Organization: University at Albany, Albany NY 12222
X-Newsreader: NNR/VM S_1.3.2
Lines: 17

One of our users is having an unusual problem.  If she does an Alt/Tab to
a full-screen DOS program, when she goes back to Windows her desktop fonts
have changed.  If she goes back to a full-screen DOS program and then goes
back to Windows, the font has changed back to its default font.  It's not
a major problem (everything works and the font is legible), but it is
annoying.  Does anyone have any idea why this happens.  By the way, she
has a DEC 486D2LP machine.


++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Jeffrey M. Cohen                      Voice: 518-442-3510
Office for Research (AD 218)          Fax:   518-442-3560
The University at Albany              E-mail: JC924@uacsc2.albany.edu
State University of New York
1400 Washington Ave.
Albany, NY 12222
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
			\end{lstlisting}
		\end{tabular}
	\end{center}
	\caption{Posting example from the comp.os.ms-windows.misc newsgroup}
\end{figure*}

\begin{table}[]
	\centering
	\caption{Postings per newsgroup}
	\label{tab:groups}
	\begin{tabular}{c|c}
		\hline                      %inserts double horizontal lines
		Newsgroup & Number of postings \\ %[0.5ex]
		\hline
		rec.motorcycles & 996 \\
		comp.sys.mac.hardware & 963 \\
		talk.politics.misc & 775 \\
		soc.religion.christian & 997 \\
		comp.graphics & 973 \\
		sci.med & 990 \\
		talk.religion.misc & 628 \\
		comp.windows.x & 988 \\
		comp.sys.ibm.pc.hardware & 982 \\
		talk.politics.guns & 910 \\
		alt.atheism & 799 \\
		comp.os.ms-windows.misc & 985 \\
		sci.crypt & 991 \\
		sci.space & 987 \\
		misc.forsale & 975 \\
		rec.sport.hockey & 999 \\
		rec.sport.baseball & 994 \\
		sci.electronics & 984 \\
		rec.autos & 990 \\
		talk.politics.mideast & 940
	\end{tabular}
\end{table}

\subsection{Feature Extraction of Text Data}

Since raw sequences of characters cannot be processed by neural networks, we need to represent our data in vector of fixed sizes. In order to achieve this, the text has been converted into the commonly used bag-of-words model after removing English stop words, such as "the" and "is". Our model only contains single words and not pair of words used next to each others\footnote{These pairs of words are commonly knows as bigrams and are used to keep information about the structure of sentences.}. Furthermore, the model does not discard words depending on their appearance of frequency. Every word used in the text is contained in the model, even if it only appears once.

TODO: example

Next, we normalize the entries in our bag-of-words representation. For classification words with a higher frequency of occurrence are usually less informative than specialized words which are more rarely used. Therefore, if we would use the frequency of occurrence for rating a word's importance, we will assign more specialized words a less important impact. This low rating causes that specialized words are considered as of less importance than general words, even though it's the opposite. To avoid this, we need to avoid that word's importance is based on its frequency of occurrence. This can be achieved by rating a word's impact based on its term frequency-inverse document frequency (TFIDF). Using this normalization, words with a high frequency will by considered of less importance.

\subsection{Deep Neural Network for Classification}

Our neural networks contains one or two hidden feed-forward layers consisting of 50 or 100 perceptrons. The learning rate is 1, 0.1, 0.001 or 0.0001. For the activation function we choose the rectified linear unit function (ReLu) as defined by $f(x) = max(0, x)$. This decision was based on previous work which showed the ReLu is able to provide better classification accuracy in supervised training than the hyperbolic tan function or the the logistic sigmoid function\cite{pmlr-v15-glorot11a}. The loss-function is optimized using the stochastic gradient descent optimizer Adam as proposed by Kingma, Diederik, and Jimmy Ba \cite{adam}. The L2 penalty paramter $alpha$ for regularization has been set to 0.0001.

Each neural network is trained until its accuracy stops to increase noticeably. For this purpose, 10\% of the training set will be split apart as a validation set. After each training iteration using the remaining 90\% the network's performance will be evaluated using the validation set. If the mean accuracy across all classes does not improve more than 0.1 percentage point for two iterations, the training will be stopped. 

\subsection{Evaluation}

The classifier's performance is measured using its mean accuracy. This is done by calculating the mean of the classifier's accuracy for each label.

In order to find the best combination of hidden layers and learning rate, we perform a grid search where for each combination of these parameters a classifier is trained and evaluated using the mean accuracy. As mentioned above, our learning rate is one of 1, 0.1, 0.001 and 0.0001 while number of layers is one or two with 50 or 100 perceptrons per layer. In total we train and evaluate the performance of 20 neural networks.

To further ensure that we do not overfit our model on our test data set, we are using 4-folded cross validation. In this method, the training set for a classifier will be split into four distinct folds. Next, four classifiers will be trained using three of the four distinct folds and one fold will be used for validation. For each classifier the validation set will be a different one. Finally, the mean accuracies of the fours classifiers is calculated which is then used as the accuracy for the entire classifier\cite{introduction_ml}.

\section{Results}

\begin{figure}
	\centering
	\label{fig:heatmap}
	\includegraphics[width=9cm]{fig2.pdf}
	\caption{Mean accuracy for a classifier using different hyperparameters. The X-axis depicts te learning rate while the Y-axis show the number of hidden layers and number of perceptron per hidden layer.}
\end{figure}

\section{Conclusion}

\bibliographystyle{ACM-Reference-Format}
\bibliography{document}

\end{document}
